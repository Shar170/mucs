import pandas as pd
import altair as alt
import numpy as np

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

import streamlit as st
models = []
best_model = None


def show_menu():
    global best_model, models
    st.write(
    """
    ## –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É–¥–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –Ω–∞ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–∏
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ .csv —Ñ–∞–π–ª —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥—Ä–æ–±–ª–µ–Ω–∏—è –∏ –∫–æ–Ω–µ—á–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º (–∫–æ–ª–æ–Ω–∫–∞ –¥–æ–ª–∂–Ω–∞ –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è `d`)

    """)

    df_file = st.file_uploader('Dataset of stable diameters', type='csv')
    if df_file is not None:
        exp = pd.read_csv(df_file)
        st.write(exp)
       
        surf_energy = st.slider('–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è', min_value=0.0001, max_value=100.0, value=0.15, step=0.001) #0.15
        density = st.slider('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å', min_value=1.0, max_value=50000.0, value=4000.0, step=100.0) #4000

        exp['eps']=exp['d'].apply(lambda x: ((((6.0*surf_energy)/density)**3.0)*((0.000001*x)**(-5.0)))**(1/2))#(6*surf_energy/density)**(3/2)*(0.000001*x)**(-5/2))
        exp_show = exp.copy()
        exp = exp.drop(['d'], axis=1)
        exp.head()
        run = st.button('–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏')
        if run:
            trg = exp[['eps']]
            trn = exp.drop(['eps'], axis=1)

            mlp_regressor = MLPRegressor(hidden_layer_sizes=(10, 5),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö
                             activation='relu',  # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
                             solver='lbfgs',  # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                             learning_rate='invscaling',  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
                             learning_rate_init=0.001,  # –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
                             alpha=0.0001,  # –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
                             max_iter=10000000)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π

            models = [  
                        Pipeline([('scaler', MinMaxScaler()),('model',  SVR(kernel='rbf', C=100, gamma=50.0, epsilon=0.0001))]),
                        ##Pipeline([('scaler', MinMaxScaler()),('model', LinearRegression(fit_intercept=False,positive=True))]), # –º–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤
                        Pipeline([('scaler', MinMaxScaler()),('model', RandomForestRegressor(n_estimators=50, max_features ='sqrt'))]), # —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å
                        Pipeline([('scaler', MinMaxScaler()),('model', KNeighborsRegressor(n_neighbors=4))]), # –º–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
                        Pipeline([('scaler', MinMaxScaler()),('model', DecisionTreeRegressor())]),
                        Pipeline([('scaler', MinMaxScaler()),('model', GradientBoostingRegressor())]),
                        Pipeline([('scaler', MinMaxScaler()),('model', mlp_regressor)]),                                                              
                        ]

            #—Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            TestModels = pd.DataFrame()
            tmp = {}
            #–¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ —Å–ø–∏—Å–∫–∞
            cont = st.empty() #
            for model in models:
                #–ø–æ–ª—É—á–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏
                m = str(model.named_steps['model'].__class__.__name__)
                tmp['Model'] = m#[:m.index('(')]    
                #–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞
                cont.success(f'{m} started')
                
                model.fit(trn, trg)
                #–≤—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏
                r2 = r2_score(trg, model.predict(trn))
                if model.named_steps['model'].__class__ is MLPRegressor and r2 < 0:
                    retry = 0
                    while(r2 < 0 and retry < 1000):
                        
                        model.fit(trn, trg)
                        r2 = r2_score(trg, model.predict(trn))
                        retry += 1

                cont.success(f'{m} finished')
                tmp['R2_eps'] = r2
                #tmp['MSE'] = mean_squared_error(trg, model.predict(trn))
                for col in range(len(trn.columns)):
                    try:
                        tmp[f'{trn.columns[col]} importance'] = model.named_steps['model'].feature_importances_[col]
                    except:
                        tmp[f'{trn.columns[col]} importance'] = None
                #–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –∏—Ç–æ–≥–æ–≤—ã–π DataFrame
                TestModels = pd.concat([TestModels, pd.DataFrame([tmp])])
            cont.success('–†–∞—Å—á—ë—Ç—ã –º–æ–¥–µ–ª–µ–π –æ–∫–æ–Ω—á–µ–Ω—ã!')
            
            #–¥–µ–ª–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
            TestModels.set_index('Model', inplace=True)

            import matplotlib.pyplot as plt

            # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            fig, axes = plt.subplots(figsize=(10, 4))

            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            TestModels.R2_eps.plot(kind='bar', title='R¬≤')

            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–¥–ø–∏—Å–µ–π –ø–æ–¥ —É–≥–ª–æ–º
            plt.xticks(rotation=45)  # –£–≥–æ–ª –ø–æ–≤–æ—Ä–æ—Ç–∞ –≤ –≥—Ä–∞–¥—É—Å–∞—Ö

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            st.pyplot(fig)

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            st.dataframe(TestModels)

            for model_index in range(len(models)):
                st.markdown("""---""")
                model = models[model_index]
                model_name = model.named_steps['model'].__class__.__name__
                # create a mesh to plot in
                h = .02  # step size in the mesh
                x_min, x_max = trn['r_shar'].min() - 1, trn['r_shar'].max() + 1
                y_min, y_max = trn['m_shar'].min() - 1, trn['m_shar'].max() + 1
                xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                    np.arange(y_min, y_max, h))
                # Plot the decision boundary. For that, we will assign a color to each
                # point in the mesh [x_min, x_max]x[y_min, y_max].
                Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
                # Put the result into a color plot
                Z = Z.reshape(xx.shape)
                fig2 = plt.figure()
                plt.contourf(xx, yy, Z, cmap='gray')#cmap=plt.cm.Paired)
                plt.title(f"–ü—Ä–µ–¥–∏—Ç–∏–≤–Ω–∞—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –¥–ª—è {model_name}")
                plt.axis('tight')

                # Plot also the training points
                colors = "bry"
                
                plt.scatter(trn['r_shar'], trn['m_shar'], cmap=plt.cm.Paired)#, c=colors[0], cmap=plt.cm.Paired)

                st.pyplot(fig2) 

                best_model_index = model_index
                best_model = models[best_model_index]
                best_model_name = best_model.named_steps['model'].__class__.__name__

                #exp_show['error'] =  exp_show[["d","d_predicted"]].apply(lambda x: )
                
                try:
                    exp_show["eps_predicted"] = best_model.predict(trn)
                    exp_show["d_predicted"] = exp_show['eps_predicted'].apply(lambda x:  (6.0*surf_energy/density)**(3/5) * 1000000 * (1/(x**(2/5))))
                    exp_show["error"] = exp_show[["d_predicted", 'd']].apply(lambda x:  f"{100* abs(x['d'] - x['d_predicted'])/x['d']:0.2f}%", axis=1)
                    r2 = r2_score(exp_show['d'],exp_show['d_predicted'])*100
                    mse = mean_squared_error(exp_show['d'],exp_show['d_predicted'])
                    st.write(f"–¢–æ—á–Ω–æ—Å—å –º–æ–¥–µ–ª–∏ {best_model_name} —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç R^2=`{r2:.2f}%` MSE=`{mse:.4f}¬µ`")
                    st.write(exp_show)
                except:
                    st.error('—Ä–∞—Å—á—ë—Ç –º–æ–¥–µ–ª–∏ –ø—Ä–∏–≤—ë–ª –∫ –Ω–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–º –¥–∞–Ω–Ω—ã–º. –û–±—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —ç—Ç–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ –≤–æ–∑–º–æ–∂–Ω–æ üò¢')
                

                    


            best_model_index = 4
            best_model = models[best_model_index]
            best_model_name = best_model.named_steps['model'].__class__.__name__

        

if __name__ == '__main__':
    show_menu()